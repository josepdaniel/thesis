\documentclass[openany]{book}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{booktabs}
\usepackage{caption} 
\usepackage{multirow}
\usepackage{fancyhdr}
\usepackage{subfiles}
\usepackage{subfig}
\usepackage{float}
\usepackage{hyperref}
\hypersetup{pdftex,colorlinks=true,allcolors=black}
\usepackage{hypcap}
% \renewcommand{\thesubfigure}{\Alph{subfigure}}



\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\fancyhf{}
% \fancyfoot[LE,RO]{\thepage}
\fancyhead[RO]{\nouppercase{\thepage}}
\fancyhead[RE]{\nouppercase{\thepage}}
\fancyhead[LE]{\slshape\nouppercase{\rightmark}}
\fancyhead[LO]{\slshape\nouppercase{\rightmark}}


\captionsetup[table]{skip=5pt}
\captionsetup{format=plain, font=small, labelfont=bf}

\makeatletter
% \renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother


\renewcommand{\baselinestretch}{1.5}

\title{\textbf{Learning Depth and Visual Odometry From Light Fields} \\
\large School of Aerospace, Mechanical and Mechatronic Engineering \\
\large The University of Sydney}
\date{May, 2020}

\author{Joseph Daniel}

\begin{document}

\frontmatter


\maketitle


\chapter*{Abstract}

The meteoric rise of mobile robotics has seen traditionally difficult-to-navigate environments like the home, the road and the ocean, become standard operating environments for autonomous machines. This rapid advancement has culminated in an increasing need for accurate robotic navigation, where the pinnacle of autonomy is the ability to operate adaptively in unconstrained environments. Central to this capability is the ability to perceive motion - a task that has recently drawn considerable attention from those in the computer vision community and given rise to a family of algorithms called \textit{visual odometry}. 

The primary contribution of this work is a novel, data-driven pipeline for visual odometry and depth perception, which combines recent successes in plenoptic imaging with the pattern recognition capabilities of convolutional networks. Notably, we formulate our algorithm as an \textit{unsupervised} learning problem, meaning neither ground-truth odometry, nor depth is used to train our pipeline. The self-supervising nature of our algorithm equips it with a robustness to effects like thermal expansion and shock, as it is capable of adapting and learning from experience, even whilst operating in situ.

We trained and validated our algorithm on a dataset collected as part of this thesis project, using a robotic arm with a known kinematic model for ground-truth validation data. Finally, we demonstrate that our algorithm outperforms existing state-of-the-art monocular approaches, demonstrating the capabilities of plenoptic imaging for robotic perception. 

\chapter*{Statement of Contribution}

\begin{itemize}
  \item I carried out the literature review in order to evaluate existing algorithms and approaches.
  \item I authored the data acquisition software and collected the dataset of light field video used in this work. 
  \item With the help of my supervisor Dr. Donald Dansereau, I designed and implemented the novel algorithms described in this work.
  \item I augmented existing code from Zhou \textit{et al.} \cite{zhou2017unsupervised}, authoring custom components for data-loading, loss computations, and performing inference.
  \item I carried out the experiments described in this work using the proposed algorithms.
  \item I authored the functions for analysing experimental results, and conducted the analysis myself, with advice provided by my supervisor. 
  \item I carried out the discussion and conclusion, which are my own, influenced by discussion with my supervisor.
\end{itemize}

\includegraphics[height=1.5in]{images/blank.png}

\textbf{Joseph Thomas Daniel} 

27th May, 2020


\chapter*{Acknowledgements}
I would like to express my gratitude to my supervisor, Dr. Donald Dansereau, for his guidance throughout this thesis. His intellect and motivation has inspired my interest in robotics and computational imaging, and his perceptiveness has at all times kept me thinking critically. My thanks also to my colleagues in the school of AMME, who have provided consistently valuable feedback in our weekly roundtable discussions and made this thesis all the more enjoyable.

I would like to also acknowledge my family and friends who have supported me during this thesis, whether through their heartening encouragement or simple acts of homemade food delivery. Finally, a special thanks to Josephine for her patience and support in this time of physical isolation - she has consistently shown her compassion and kindness, even though the apartment we share certainly wasn't designed for two to be working from home.

\tableofcontents
\listoffigures
\listoftables

\mainmatter
\subfile{chap1_intro.tex}
\subfile{chap2_background.tex}
\subfile{chap3_lit.tex}
\subfile{chap4_methods.tex}
\subfile{chap5_results.tex}
\subfile{chap6_discussion.tex}
\subfile{chap7_conclusion.tex}


\newpage

%\newpage
\bibliographystyle{plain}
\bibliography{bibliography.bib}

\end{document}
