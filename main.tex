\documentclass[openany]{book}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{booktabs}
\usepackage{caption} 

\usepackage{fancyhdr}
\usepackage{subfiles}

\renewcommand{\headrulewidth}{0pt}
\pagestyle{fancy}
\fancyhf{}
% \fancyfoot[LE,RO]{\thepage}
\fancyhead[RO]{\nouppercase{\thepage}}
\fancyhead[RE]{\nouppercase{\thepage}}
\fancyhead[LE]{\slshape\nouppercase{\rightmark}}
\fancyhead[LO]{\slshape\nouppercase{\rightmark}}


\captionsetup[table]{skip=5pt}

\makeatletter
% \renewcommand{\@chapapp}{}% Not necessary...
\newenvironment{chapquote}[2][2em]
  {\setlength{\@tempdima}{#1}%
   \def\chapquote@author{#2}%
   \parshape 1 \@tempdima \dimexpr\textwidth-2\@tempdima\relax%
   \itshape}
  {\par\normalfont\hfill--\ \chapquote@author\hspace*{\@tempdima}\par\bigskip}
\makeatother


\renewcommand{\baselinestretch}{1.5}

\title{Learning Depth and Visual Odometry From Light Fields \\
\large School of Aeronautical, Mechanical and Mechatronic Engineering \\
\large The University of Sydney}

\author{Joseph Daniel}

\begin{document}

\maketitle

\chapter*{Declaration of Contribution}
I did some things.

\chapter*{Abstract}

The meteoric rise of mobile robotics has seen traditionally difficult-to-navigate environments like the home, the road and the ocean, become standard operating environments for autonomous machines. This rapid advancement has culminated in an increasing need for accurate robotic navigation, where the pinnacle of autonomy is the ability to operate adaptively in unconstrained environments. Central to this capability is the ability to sense motion - a task that has recently drawn considerable attention from those in the computer vision community and given rise to a family of algorithms called \textit{visual odometry}. 

The primary contribution of this work is a novel, data-driven pipeline for visual odometry, which combines recent successes in plenoptic imaging with the pattern recognising capabilities of convolutional neural networks. While the algorithm is indeed a data-driven one, the proposed pipeline is unsupervised, meaning ground truth data which is typically expensive and time-consuming to acquire, is not required for the algorithm to function. Furthermore, the unsupervised nature of the algorithm endows it with a robustness to the inconvenient, calibration-wrecking effects that field robotics are all-too-frequently subjected to, such as thermal expansion and shock. 

The proposed algorithm is trained and validated on a dataset which was collected over the course of this project. A robotic arm with a known kinematic model is used for data-collection, providing ground truth trajectory data for validation. While the algorithm does not require this ground truth data to train on, it is nevertheless a useful byproduct of the data-acquisition process, allowing us to perform a healthy validation study of the proposed model.

Finally, the proposed algorithm is compared to two existing families of visual odometry algorithms. The first is a group of algorithms that directly model the geometry of multi-view imaging, solving directly for ego-motion using a closed-form solution. The second family uses a data-driven approach, much like the algorithm proposed in this work, but unlike this work they do not harness the power of plenoptic imaging.


\chapter*{Acknowledgements}
Thanks Don.





\tableofcontents
\listoffigures
\listoftables


\subfile{chap1_intro.tex}
\subfile{chap2_background.tex}
\subfile{chap3_lit.tex}
\subfile{chap4_methods.tex}


\newpage

%\newpage
\bibliographystyle{plain}
\bibliography{bibliography.bib}

\end{document}
