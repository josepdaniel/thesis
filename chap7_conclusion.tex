\chapter{Conclusions and Future Work}


\section{Conclusions}
Throughout this work, we have advocated for robust, data-driven, computer-vision algorithms which will enable the deployment of autonomous robotics in unconstrained environments. 

In this work we identified an opportunity to address a knowledge gap - namely the ability to perform visual odometry using an uncalibrated camera array. Meanwhile, we have blended the successes of two promising technologies - light field imaging and deep learning - to address the visual odometry problem.  Not only does our proposed algorithm build robustness through experience, but it has been designed to operate on minimal prior knowledge of its parent system, adapting to adverse environments and improving performance \textit{in situ}. 

We continue to uphold the belief that computational imaging - the co-design of optics and algorithms - will continue to propel advancements in imaging technology for the foreseeable future. Most of us need not look any further than our own pockets to find a piece of technology that is emblematic of the progress computational imaging has made. Since the outset of this thesis, it has become more-or-less standard practice for mobile phone producers to embed two or more camera sensors on the back of their phones - even in the budget variety. 

\subsection{Usages in Robotics}
We envisage a number of potential applications for our algorithm. Primarily, it has been designed as an odometry component for autonomous systems, simplifying the process of navigation, localisation and mapping. Robotic systems rarely operate in the absence of feedback data, especially in intelligent, mobile systems. Where robotics is concerned, we frequently hear about sensor-fusion - a simple and practical way of combining multiple sources of truth to build the best probabilistic picture of the robot's operating environment. A simple application is hovering-in-place using an AUV in a GPS denied area - a problem that in any practical scenario cannot be solved with dynamic system models alone, requiring some form of feedback data. Feedback using visual odometry is good, but feedback using a fusion of visual odometry and inertial measurements for example would be better. 

\subsection{Usages Outside of Robotics}
While we have primarily focused on robotics, we also recognise that as computational imaging becomes more pervasive in consumer electronics, numerous potential applications of our algorithm emerge. For example, the ability to track camera motion while simultaneously reconstructing a 3D model of the scene has applications in Augmented Reality, enabling 3D models to be rendered over the top of a real world scene. Similarly, the ability to predict camera motion from one frame to another may be employed to smooth motion in a shaky video. In a similar vein, video frames may even be interpolated using the photometric warp technique that we have used for training our models - effectively increasing the frame rate of the footage.

\section{Future Work}

There are numerous exciting avenues of research to branch off this work, which we describe in the following. In suggesting these research questions, we begin with more immediate extensions of the present work, and build up to longer-term research goals. 

\subsection{Real-time Inference and Online Learning}
We have suggested throughout this work that our algorithm is capable of online learning. A simple yet exciting extension of this work is to implement a system that is capable of such online learning, perhaps on a mobile robotic platform or visually equipped quad-rotor. Depending on the implementation requirements, this may mean minifying the computational cost of the algorithm to enable training on the edge device, or alternatively modifying the training routine to operate in the cloud whilst collecting data simultaneously from several devices. Deployment options may also be explored - weighing up the comparable advantages of inference at the edge versus in the cloud. Devices such as the Coral and Jetson Nano have seen `edge' computing rise in popularity, while the scalability of cloud platforms has similarly made their services very popular. 

\subsection{Fusion and Filtering}
Light field imaging is just one expression of computational imaging - meanwhile, RGB-D sensors, lidar and time-of-flight cameras can, and should be used to expand the versatility of our system. Current limitations of our algorithm - such as the challenges presented by low-texture materials - may be partially addressed by the fusion of multiple visual sensors. This makes sense especially in larger robotic platforms such as autonomous vehicles and industrial robotics - where safety and robustness are key. In a similar vein of research, we suggest that integration with existing SLAM algorithms is a useful research direction that will more rapidly enable the adoption of such algorithms in mobile robotics.

\subsection{Deep Learning and Light Field Imaging}
Deep learning is no longer simply a niche research area, with software libraries and hardware-accelerated devices making it a research area that is more accessible than ever. Similarly we are experiencing a maturing of light field imaging technology, with applications in robotics, consumer electronics and industry. Applications which have previously found success applying deep learning to conventional imagery or footage now stand to also benefit from the rich 3D data captured by light field cameras. 