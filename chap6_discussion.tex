\chapter{Discussion}

In Chapter 4, we hypothesised that of our two suggested learning pipelines, the variant that photometrically reconstructs the complete light field is likely to out-perform either the monocular or single-view-reconstruction approach. Our experimental results in Chapter 5 largely confirmed this hypothesis, and in this chapter we investigate these results in more detail, drawing conclusions from our data and relating these to our hypothesis. 

Not only were our suspicions with respect to the learning pipeline confirmed, but we also showed that of the three insertion methods, using tiled epipolar plane images produced the best pose-estimates and the smallest photometric warp error. We present a discussion in this chapter around the impact that different insertion methods have.

We recognise that deep neural networks are opaque in nature - their millions of parameters makes any meaningful understanding of their internal mechanisms challenging to understand. However, we do know from decades of machine learning research that convolutional networks act as feature-extractors - and so our methodology has focused on providing data to these networks with the most informative features presented first. Regardless, what we present in this chapter is only a reasoned understanding of the most likely impacts of the different input formats and learning pipelines.  

The first part of this chapter discusses the compromises between our three unsupervised learning variants. The second part compares the different insertion methods, investigating the likely causes for the disparate results between them. Finally, we compare our results to state-of-the-art monocular approaches. 


\section{Comparison Between Unsupervised Learning Pipelines}

In this section, we present a discussion on the different impacts of the two unsupervised learning pipelines (monocular, single-view-reconstruction and light-field-reconstruction). Specifically, we address four trade-offs which impact the performance of our convolutional networks - the strength of the supervision signal, the enforcement of scale consistency, pipeline flexibility, and the computational cost. 

\subsection{Strengthening the Supervision Signal}
One question that remained, prior to the analysis of our results, was whether the photometric reconstruction of the entire light-field might lead to the unstable propagation of errors through our network. As the number of pixels being reconstructed increases, so to does the magnitude of the loss function. As the loss signal grows, there remains a possibility that the partial derivatives of the network's parameters with respect to the loss grow large, causing the optimisation algorithm to apply unstable updates to the network's parameters. We had prepared strategies for mitigating this instability in the case that it did occur, such as reducing the learning-rate and gradient-clipping, however we found that training progressed smoothly using the light-field reconstruction approach. We conclude from this that not only is photometrically reconstructing the complete light-field a stable formation of a loss function, but it is indeed beneficial to the learning of depth and visual odometry. Supporting this conclusion, we observe that the light-field-reconstruction pipeline supports our networks in locating a lower local minimum, with an average of a 10.3\% reduction in photometric error compared to single-view-reconstruction across all our experiments. 

Importantly, we distinguish between simply magnifying the supervision signal, and using light-field-reconstruction to obtain a heavier loss function. If we relied on the former strategy, we might simply increase the learning-rate to penalise errors more harshly - which leads to instability in training. We conclude that by photometrically reconstructing the 4D light field, we teach our networks something far more useful about geometry and the light field that enables them to locate a smaller minimum in the topographic loss landscape.

\subsection{Enforcing Scale Consistency in Translation and Rotation}
Aside from applying a heavier supervision loss signal to the two networks, the light-field-reconstruction method also penalises scale inconsistencies. We discussed in Chapters 3 and 4 the weaknesses of monocular approaches - because the information used to generate the photometric warp is sourced from a single pinhole view of the scene, we are restricted to 2D data. When adjusting the approach to reconstruct the entire 4D light field however, our networks are required to learn the relationship between the sampled image data and the geometry of the scene. In the monocular case, a depth network in the absence of better information will happily estimate a twice-as-large room, if of course the pose network estimates a twice-as-large translation. Notably however, this does not extend to rotations - the image formed by scaling the rotation of the camera is not supplemented by scaling the magnitude of the depth map. It is surprising then, that the light-field-reconstruction method produces the largest boost in performance in terms of rotation, not in translation. We observe that when using the light-field-reconstruction pipeline, the rotation error estimate is on average 39\% better across all input methods. Translational errors on the other hand experienced only a 9.7\% performance improvement.

We speculate that this performance boost arises from the combination of a stronger supervision signal and the enforcing of scale consistency through the 4D warp. The stronger supervision signal helps the two convolutional networks find a smaller local minimum in the loss topography, while the scale-consistency constraint provides improved certainty in the scale of the translation, the geometry of the scene, the relationship between sub-apertures, and the ability to resolve depth in the scene. Rotational errors are more likely to exacerbate the photometric loss compared to translational errors, and through this combination of factors, we believe that our pose network has learned a more robust function for estimating rotations. 

\subsection{Compromising on Pipeline Flexibility}
An important capability that arises from our pipeline is the ability to perform visual odometry and depth estimation using an \textit{uncalibrated} camera array. The attractiveness in this approach is that the algorithm is suitable for use in mobile robotics which are frequently subjected to temperature change, vibration and shock. This ideally means that we have minimal prior knowledge about the camera being used - however we acknowledge that the light-field-reconstruction pipeline sacrifices some flexibility, as this variant requires prior knowledge of the \textit{shape} of the camera being used. However, we also argue that even in the light-field-reconstruction case, the absolute magnitude of the relationship between sub-apertures are not required. While depth and pose estimates will indeed be adversely affected by these phenomena, we argue that even as the calibration error of the camera array accumulates, the `best' solution to the photometric-warp problem (i.e. the global minimum for the photometric warp loss) - which our neural networks are tasked with solving - converges to the correct pose and depth estimate. This is an error that cannot be ignored using stereo or even trinocular imagery, but as the number of sub-apertures increase, the `best' photometric warp solution converges to that generated using the correct pose and depth. 

\subsection{Computational Cost}
Both of our suggested pipelines operate on arbitrarily large images, meaning images may be cropped and scaled, as long as the intrinsic matrix $K$ is also scaled appropriately. The execution time for inference using both algorithms is predictable and repeatable as long as the dimensionality of the input remains constant, even as the model is trained and fine-tuned on new data. This is an attractive capability in robotics, where prior knowledge of an algorithm's bandwidth greatly simplifies system design. 


\section{Comparison between Input Methods}
One of the more exciting investigations to be conducted in this work is on the utility of different representations of the light field for neural networks in geometry-based tasks. In this section we compare the performance of our three insertion strategies, and speculate on the success of the different methods. Previously, we described the challenge as finding the best way of organising 4D data, as 2D slices in an informative and meaningful way for learning depth and visual odometry. Experimentally, we found that the use of tiled epipolar plane images produced the best performance of our three tested methods.

\subsection{Performance of Epipolar Plane  Images}

Epipolar plane images explicitly encode depth in the `slope' of their characteristic sheared lines, which is known as the gradient-depth constraint. Not only does this provide an explicit feature-set for a neural network to learn geometry from, but the derivatives of this 4D space when extended into the time domain allows motion to be directly inferred, as we saw in the \textit{plenoptic flow} algorithm.

We attribute the success of this input format to the clear expression of depth information, through simple 2D features which are easily decoded through the 2D convolution of signals. Furthermore, our method includes information from three different feature spaces - in \textit{U, V}, as well as \textit{S, U} and \textit{T, V}. Now \textit{S, U} and \textit{T, V} are similar feature spaces in that the depth information contained in both represents the same scene geometry. \textit{U, V} slices on the other hand depict a pinhole-projection of the scene, allowing for the rich textural details and semantic features of the scene to be learnt. By combining these feature spaces, we suggest that our method is capable of learning to approximate the 4D signal processing functions required for performing depth and pose estimation.

Recall that our EPI insertion strategy includes the use of a feature extractor which preprocesses the tiled EPI prior to feeding to our depth and pose networks. The purpose of this encoder module was to down-sample the resolution of our tiled images. We recognise however that this is an additional `learnable' module that effectively increases the number of learnable parameters in both our depth and pose networks. 


\subsection{Performance of Volumetric Stacks}

Stacking the \textit{U, V} slices volumetrically is a simple way of formatting the light field data - it's formation is easy to understand, and the computational cost is relatively cheap. This format contains raw, unprocessed data from the light field. In performance, we found that volumetrically stacking the images exhibited poorer performance than either tiled EPIs or focal stacks. We suggest that the performance did not match the other formats for two reasons. Firstly, the data is unprocessed and does not re-slice the light field to present any meaningful geometric features first - instead this approach is a simple extension of the monocular case, including several monocular views of the scene in the channels dimension. It is possible that a 2D convolutional filter may learn features relating to parallax and occlusion through the channel dimension, but the convolutions take place in the `angular' dimension rather than the `spatial' dimension. Secondly, our tiled EPI input format has a clear advantage over either of our two other formats in that it is able to take advantage of 3 feature spaces. Volumetrically stacking the images on the other hand, only allows our networks to learn simple 2D convolutions in a single feature space. 

\subsection{Performance of Focal Stacks}
Focal stacks are highly configurable - aside from choosing the number and the depth of the focal planes comprising the stack, the depth-of-field may also be configured by varying the number of sub-apertures used to construct it. 

In our experiments we tested two variants - in both cases we employed all sub-apertures (this has the effect of making a very thin focal plane), and tested the results when focusing at 5 versus 9 planes. To our surprise, the 5-plane experiment exhibited the superior odometry results. Our expectations for this input format are that objects' depths are encoded in the focal-plane which they occupy - i.e. whether they are resolved clearly or blurrily at a specific focal depth. 

This is a surprising result, as we expect the number of possible depth encodings to be greater with the 9-plane variant. We suggest three ideas that may have given rise to this result. The first is a matter of experimental setup - the specific depth planes that were chosen to construct the stack. It may be the case that our choice of 9 focal planes simply introduced redundant information that was already encapsulated in the 5-plane data, making it more difficult for the network to select useful features. Secondly, the construction of focal stacks assumes a texturally rich scene - it is difficult to tell whether regions of low texture are in or out of focus. Thirdly, as we have seen, using sparse camera arrays for synthetic aperture focusing is prone to aliasing, creating artifacts in the image data - which is clearly visible in Figure 4.6. With a larger number of focal planes being used, the volume of artifacts, and potentially detrimental non-existent features interpreted by our depth and pose networks increases.

While we have suggested these three ideas for how this result may have arisen, we believe it is most likely a combination of several factors. We remain unconvinced that the provision of additional data through focus stacking is a detrimental factor in learning geometry-related tasks, and recommend future work to investigate more precisely the impacts of varying the number of planes, the depth-of-field, and the types of scenes being imaged.

\section{Comparison to Existing Monocular Approaches}

Our experimental data indicates that, unsurprisingly, our approach using light field data outperforms the state-of-the-art monocular approach described in \cite{zhou2017unsupervised}. We expected this to be the case, as the data we have collected and provided to our networks is far more informative than simple monocular imagery when used for geometry based learning. Our models outperform the monocular approach not only in depth and odometry errors, but also in the average photometric warp error - each image that is synthesised is a better match for the target image using our pipeline than in the monocular pipeline. 

Not only are the pose errors and photometric losses lower using light field data, but we also show empirically show that it exhibits improved scale consistency, and metric depth and pose estimates.
