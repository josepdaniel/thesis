\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of robots that operate in unconstrained spaces.}}{11}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The pinhole model of the camera}}{17}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces The fundamental matrix in epipolar geometry}}{18}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces The two-plane parameterisation and the free-space assumption}}{19}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of a camera array mounted on a robotic arm. This camera array is configured as 17 sub-apertures arranged on a single plane in a cross-hair formation. Camera arrays sample several views of the same scene and are thus capable of acquiring a sparse sample of the light field. This is the camera that will be used throughout this thesis project.\relax }}{21}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Epipolar Plane Images (left): Shown as a slice of a volume, the images formed by dissecting the image in the \textit {s,u} and \textit {t,v} planes are characterised by sheared straight lines, with the grade of the slope encoding the amount of parallax experienced by a pixel at that \textit {u} or \textit {v} coordinate. Synthetic aperture focusing (right): taking the average of every image from the camera array yields an image where different parts are in focus depending on the alignments of the images.\relax }}{21}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Multilayer Perceptron (left): each output from one layer is fed into the inputs of every node in the subsequent layer. Convolutional Neural Networks (middle) on the other hand have a 'receptive field', taking advantage of the spatial coherence of pixels in image data. The convolutional upsampling operation (right) is frequently used to upsample a low-dimensional feature space, to a higher dimensional one. It is often employed as a 'learned' information decompression.\relax }}{23}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Machine learning approaches have demonstrated strong results in simultaneously estimating depth maps and relative poses between images. Typically, a pair of CNN's is used, one for each task. The depth CNN is provided with the first view at $t=1$ and the pose CNN is provided both views.\relax }}{30}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces To provide a supervisory signal to the depth and pose CNN's, Garg et al. \cite {garg2016unsupervised} suggested using photometric reconstruction. The loss function is formulated by taking the difference between the reconstructed image $\mathaccentV {hat}05E{I_{t1}}$ and the actual image $I_{t1}$, shown in equation 3.1\hbox {}.\relax }}{32}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The experimental setup utilises a Universal Robots UR5E robotic arm to precisely measure ground-truth pose to allow effective evaluation of the projects visual odometry results.\relax }}{35}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces (Left) The DispNet architecture uses a convolution encoder-decoder network characterised by skip connections, and outputs predictions at multiple scales. The multi-scale predictions aid in handling low-texture regions of the image. (Right) The PoseNet architecture uses a series of convolutional downsampling operations, predicting a final 6 degree-of-freedom pose estimate at its output activations.\relax }}{37}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Examples of trajectories generated from ground truth and estimated poses. These trajectories are generated over snippets of 40 frames from three test sequences which were withheld during training of the model. The cumulative nature of the error becomes more apparent the further the vehicle strays from the origin.\relax }}{40}% 
