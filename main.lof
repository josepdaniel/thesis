\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of robots that operate in unconstrained spaces.}}{2}{figure.1.1}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The pinhole model of the camera}}{8}{figure.2.1}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces The fundamental matrix in epipolar geometry}}{9}{figure.2.2}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces The two-plane parameterisation and the free-space assumption}}{10}{figure.2.3}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of a camera array mounted on a robotic arm. This camera array is configured as 17 sub-apertures arranged on a single plane in a cross-hair formation. Camera arrays sample several views of the same scene and are thus capable of acquiring a sparse sample of the light field. This is the camera that will be used throughout this thesis project.\relax }}{12}{figure.2.4}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Epipolar Plane Images (left): Shown as a slice of a volume, the images formed by dissecting the image in the \textit {s,u} and \textit {t,v} planes are characterised by sheared straight lines, with the grade of the slope encoding the amount of parallax experienced by a pixel at that \textit {u} or \textit {v} coordinate. Synthetic aperture focusing (right): taking the average of every image from the camera array yields an image where different parts are in focus depending on the alignments of the images.\relax }}{12}{figure.2.5}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Multilayer Perceptron (left): each output from one layer is fed into the inputs of every node in the subsequent layer. Convolutional Neural Networks (middle) on the other hand have a `receptive field', taking advantage of the spatial coherence of pixels in image data. The convolutional upsampling operation (right) is frequently used to upsample a low-dimensional feature space, to a higher dimensional one. It is often employed as a `learned' information decompression.\relax }}{14}{figure.2.6}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Machine learning approaches have demonstrated strong results in simultaneously estimating depth maps and relative poses between images. Typically, a pair of CNN's is used, one for each task. The depth CNN is provided with the first view at $t=1$ and the pose CNN is provided both views.\relax }}{22}{figure.3.1}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces To provide a supervisory signal to the depth and pose CNN's, Garg et al. \cite {garg2016unsupervised} suggested using photometric reconstruction. The loss function is formulated by taking the difference between the reconstructed image $\mathaccentV {hat}05E{I_{t1}}$ and the actual image $I_{t1}$, shown in equation \ref {photometricloss}.\relax }}{24}{figure.3.2}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Angular Filters (left): by fixing the coordinate \textit {(u, v)} and varying \textit {(s, t)}, the 2D grid of images can be presented to a neural network in the form of orthographic images. Spatial Filters (right): instead of fixing \textit {(u, v)} coordinates, \textit {(s, t)} is fixed while \textit {(u, v)} is varied. This presents information to the neural network as a grid of perspective images.\relax }}{26}{figure.3.3}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Orthographic projection (left) \cite {behrendt2006isometric}: the lines of sight to all points in the scene are parallel to one other, and perpendicular to the projective plane. Perspective projection (right) \cite {gothe2009perspective}: lines of sight converge to a point giving the effect that distant objects appear smaller than nearby objects.\relax }}{26}{figure.3.4}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The experimental setup utilises a Universal Robots UR5E robotic arm to precisely measure ground-truth pose to allow effective evaluation of the projects visual odometry results.\relax }}{29}{figure.4.1}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Tiled \textit {U, V} slices (top): this is an intuitive way of slicing the light field, which tiles images from each sub-aperture side-by-side. Tiled \textit {T, V} slices (bottom): somewhat less intuitively, this method instead tiles the vertical epipolar-plane images side-by-side. We call these \textit {wide} EPIs. As shown, the overall dimensionality and shape of the light field is unchanged, we simply present the information in a different order, revealing different encodings of the captured scene. We can also tile the light field as \textit {S, U} slices, stacking them vertically, which we refer to as \textit {tall} EPIs.\relax }}{36}{figure.4.2}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Our proposed encoder module which encodes the tiled EPIs, downsampling them to the shape expected by our depth and pose networks. Notably, features which have been learned in \textit {T, V} as well as \textit {S, U} are concatenated with the stacked \textit {U, V} slices (exactly as seen in our first input-method.) For brevity, we have only shown a small patch of the tall and wide EPIs, but the reader is reminded that these tilings are in fact very tall, and very wide.\relax }}{36}{figure.4.3}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Given knowledge of how one sub-aperture has moved through space, we can also compute how any other sub-aperture has moved by chaining the transforms in the order shown.\relax }}{38}{figure.4.4}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Estimated and true trajectories for input sequence 16.\relax }}{41}{figure.5.1}% 
\contentsline {figure}{\numberline {5.2}{\ignorespaces Estimated and true trajectories for input sequence 44.\relax }}{41}{figure.5.2}% 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Tiled EPIs}}}{41}{subfigure.1.1}% 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Focal Stacks}}}{41}{subfigure.1.2}% 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Volumetric Images}}}{41}{subfigure.1.3}% 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Tiled EPIs}}}{41}{subfigure.2.1}% 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Focal Stacks}}}{41}{subfigure.2.2}% 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Volumetric Images}}}{41}{subfigure.2.3}% 
\contentsline {figure}{\numberline {5.3}{\ignorespaces Estimated and true trajectories for input sequence 16.\relax }}{42}{figure.5.3}% 
\contentsline {figure}{\numberline {5.4}{\ignorespaces Estimated and true trajectories for input sequence 44.\relax }}{42}{figure.5.4}% 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Tiled EPIs}}}{42}{subfigure.3.1}% 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Focal Stacks}}}{42}{subfigure.3.2}% 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Volumetric Images}}}{42}{subfigure.3.3}% 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Tiled EPIs}}}{42}{subfigure.4.1}% 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Focal Stacks}}}{42}{subfigure.4.2}% 
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {Volumetric Images}}}{42}{subfigure.4.3}% 
\contentsline {figure}{\numberline {5.5}{\ignorespaces Estimated and true trajectories using monocular imagery.\relax }}{43}{figure.5.5}% 
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {Sequence 16}}}{43}{subfigure.5.1}% 
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {Sequence 44}}}{43}{subfigure.5.2}% 
\addvspace {10\p@ }
