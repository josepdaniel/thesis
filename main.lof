\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Examples of robots that operate in unconstrained spaces.}}{2}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The pinhole model of the camera}}{8}% 
\contentsline {figure}{\numberline {2.2}{\ignorespaces The fundamental matrix in epipolar geometry}}{9}% 
\contentsline {figure}{\numberline {2.3}{\ignorespaces The two-plane parameterisation and the free-space assumption}}{10}% 
\contentsline {figure}{\numberline {2.4}{\ignorespaces An example of a camera array mounted on a robotic arm. This camera array is configured as 17 sub-apertures arranged on a single plane in a cross-hair formation. Camera arrays sample several views of the same scene and are thus capable of acquiring a sparse sample of the light field. This is the camera that will be used throughout this thesis project.\relax }}{12}% 
\contentsline {figure}{\numberline {2.5}{\ignorespaces Epipolar Plane Images (left): Shown as a slice of a volume, the images formed by dissecting the image in the \textit {s,u} and \textit {t,v} planes are characterised by sheared straight lines, with the grade of the slope encoding the amount of parallax experienced by a pixel at that \textit {u} or \textit {v} coordinate. Synthetic aperture focusing (right): taking the average of every image from the camera array yields an image where different parts are in focus depending on the alignments of the images.\relax }}{12}% 
\contentsline {figure}{\numberline {2.6}{\ignorespaces Multilayer Perceptron (left): each output from one layer is fed into the inputs of every node in the subsequent layer. Convolutional Neural Networks (middle) on the other hand have a `receptive field', taking advantage of the spatial coherence of pixels in image data. The convolutional upsampling operation (right) is frequently used to upsample a low-dimensional feature space, to a higher dimensional one. It is often employed as a `learned' information decompression.\relax }}{14}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Machine learning approaches have demonstrated strong results in simultaneously estimating depth maps and relative poses between images. Typically, a pair of CNN's is used, one for each task. The depth CNN is provided with the first view at $t=1$ and the pose CNN is provided both views.\relax }}{22}% 
\contentsline {figure}{\numberline {3.2}{\ignorespaces To provide a supervisory signal to the depth and pose CNN's, Garg et al. \cite {garg2016unsupervised} suggested using photometric reconstruction. The loss function is formulated by taking the difference between the reconstructed image $\mathaccentV {hat}05E{I_{t1}}$ and the actual image $I_{t1}$, shown in equation 3.1\hbox {}.\relax }}{24}% 
\contentsline {figure}{\numberline {3.3}{\ignorespaces Angular Filters (left): by fixing the coordinate \textit {(u, v)} and varying \textit {(s, t)}, the 2D grid of images can be presented to a neural network in the form of orthographic images. Spatial Filters (right): instead of fixing \textit {(u, v)} coordinates, \textit {(s, t)} is fixed while \textit {(u, v)} is varied. This presents information to the neural network as a grid of perspective images.\relax }}{26}% 
\contentsline {figure}{\numberline {3.4}{\ignorespaces Orthographic projection (left) \cite {behrendt2006isometric}: the lines of sight to all points in the scene are parallel to one other, and perpendicular to the projective plane. Perspective projection (right) \cite {gothe2009perspective}: lines of sight converge to a point giving the effect that distant objects appear smaller than nearby objects.\relax }}{26}% 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces The experimental setup utilises a Universal Robots UR5E robotic arm to precisely measure ground-truth pose to allow effective evaluation of the projects visual odometry results.\relax }}{29}% 
\contentsline {figure}{\numberline {4.2}{\ignorespaces Tiled \textit {U, V} slices (top): this is an intuitive way of slicing the light field, which tiles images from each sub-aperture side-by-side. Tiled \textit {T, V} slices (bottom): somewhat less intuitively, this method instead tiles the vertical epipolar-plane images side-by-side. We call these \textit {wide} EPIs. As shown, the overall dimensionality and shape of the light field is unchanged, we simply present the information in a different order, revealing different encodings of the captured scene. We can also tile the light field as \textit {S, U} slices, stacking them vertically, which we refer to as \textit {tall} EPIs.\relax }}{35}% 
\contentsline {figure}{\numberline {4.3}{\ignorespaces Our proposed encoder module which encodes the tiled EPIs, downsampling them to the shape expected by our depth and pose networks. Notably, features which have been learned in \textit {T, V} as well as \textit {S, U} are concatenated with the stacked \textit {U, V} slices (exactly as seen in our first input-method.) For brevity, we have only shown a small patch of the tall and wide EPIs, but the reader is reminded that these tilings are in fact very tall, and very wide.\relax }}{36}% 
\contentsline {figure}{\numberline {4.4}{\ignorespaces Given knowledge of how one sub-aperture has moved through space, we can also compute how any other sub-aperture has moved by chaining the transforms in the order shown.\relax }}{37}% 
